# Dynamic Visual Expectations  

**Data Author**  
Andrew Marin    
amarin@ucsd.edu  

**Experimental location**   
UCSD, San Diego  


**Task Description**  
The primary goal of this experiment was to explore how dynamic visual input influences early auditory processing, particularly in the context of multisensory integration. In this study, participants viewed a ball that started at one side of the screen and traveled across the horizontal midline toward the opposite boundary. When the ball reached the boundary, a knocking sound was played. The experiment included multiple conditions designed to manipulate the relationship between visual and auditory events. In the **AV-synchronous condition**, the knocking sound occurred precisely when the ball made contact with the boundary. In the **AV-asynchronous condition**, the sound was presented slightly before the ball reached the boundary. The **audio-only condition** presented the knocking sound without visual input, while the **visual-only condition** showed the ball’s movement without any accompanying sound. Additionally, in **AV-occluded**, the ball’s motion was visually occluded before reaching the boundary, but the expected knocking sound was still presented, allowing the researchers to test how visual expectations influenced auditory processing. The auditory-evoked event-related potentials (ERPs), specifically the N1 and P2 components, were analyzed to assess the brain's response to these stimuli. The study sought to investigate how expectations from visual motion affect the early stages of auditory processing, and whether multisensory integration could facilitate this process.


**Participant categories**  
There were two groups of participants in the study. The first group consisted of 29 adults in Experiment 1, who experienced different conditions including AV-synchronous, AV-asynchronous, audio-only, and visual-only trials. The second group, made up of 19 adults, participated in Experiment 2 and experienced the AV-occluded condition.

**Data Handling**  
The raw EEG data in this study underwent thorough preprocessing to ensure high quality for analysis. First, the data were filtered with a 0.05 to 50 Hz bandpass and a 60 Hz notch filter to remove noise. Manual inspection followed to identify and address bad channels, as well as artifacts caused by eye movements and muscle activity. Channels with issues were replaced using spherical spline interpolation, and independent component analysis (ICA) was applied to remove eye-related artifacts. Additionally, any high-frequency harmonic components identified in ICA were removed. The EEG data were segmented into 1000 ms epochs, and baseline correction was performed using the 200 ms pre-stimulus period. After artifact rejection, the data were re-filtered with a 30 Hz lowpass filter and re-referenced to an average reference. The processed data were then averaged to produce grand-averaged event-related potentials (ERPs) for each condition, with specific attention given to N1 and P2 components, as defined by peak amplitudes and latencies. Further preprocessing included the application of a 1


**Publication**     
https://www.sciencedirect.com/science/article/pii/S0010945221002859     

**Data**   
https://osf.io/d245g/    

**Contact Person**  
Scott Huberty: 
